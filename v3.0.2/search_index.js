var documenterSearchIndex = {"docs":
[{"location":"rareevents/#Rare-events","page":"Rare events","title":"Rare events","text":"","category":"section"},{"location":"rareevents/#Return-time-statistics","page":"Rare events","title":"Return time statistics","text":"","category":"section"},{"location":"rareevents/","page":"Rare events","title":"Rare events","text":"mean_return_times\nexit_entry_times","category":"page"},{"location":"rareevents/#ChaosTools.mean_return_times","page":"Rare events","title":"ChaosTools.mean_return_times","text":"mean_return_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → τ, c\n\nReturn the mean return times τ, as well as the amount of returns c, for subsets of the state space of ds defined by u₀, εs. The ds is evolved for a maximum of T time.\n\nThis function is a convenience wrapper around calls to exit_entry_times and then to transit_return and then some averaging. Thus see exit_entry_times for the meaning of u₀ and εs and further info.\n\n\n\n\n\n","category":"function"},{"location":"rareevents/#ChaosTools.exit_entry_times","page":"Rare events","title":"ChaosTools.exit_entry_times","text":"exit_entry_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → exits, entries\n\nCollect exit and entry times for balls or boxes centered at u₀ with radii εs, in the state space of the given dynamical system. Return the exit and (re-)entry return times to the set(s), where each of these is a vector containing all collected times for the respective ε-radius set, for ε ∈ εs. The dynamical system is evolved up to T total time.\n\nUse transit_return_times(exits, entries) to transform the output into transit and return times, and see also mean_return_times.\n\nThe keyword show_progress displays a progress bar. It is false for discrete and true for continuous systems by default.\n\nDescription\n\nTransit and return time statistics are important for the transport properties of dynamical systems[Meiss1997] and can be connected with fractal dimensions of chaotic sets[Boev2014].\n\nThe current algorithm collects exit and re-entry times to given sets in the state space, which are centered at the state u₀. The system evolution always starts from u₀ and the initial state of ds is irrelevant. εs is always a Vector.\n\nSpecification of sets to return to\n\nIf each entry of εs is a real number, then sets around u₀ are nested hyper-spheres of radius ε ∈ εs. The sets can also be hyper-rectangles (boxes), if each entry of εs is a vector itself. Then, the i-th box is defined by the space covered by u0 .± εs[i] (thus the actual box size is 2εs[i]!). In the future, state space sets will be specified more conveniently and a single argument sets will be given instead of u₀, εs.\n\nThe reason to input multiple εs at once is purely for performance optimization (much faster than doing each ε individually).\n\nDiscrete time systems\n\nFor discrete systems, exit time is recorded immediatelly after exitting of the set, and re-entry is recorded immediatelly on re-entry. This means that if an orbit needs 1 step to leave the set and then it re-enters immediatelly on the next step, the return time is 1.\n\nContinuous time systems\n\nFor continuous systems, a steppable integrator supporting interpolation is used. The way to specify how to estimate exit and entry times is via the keyword crossing_method whose values can be:\n\nCrossingLinearIntersection(): Linear interpolation is used between integrator steps and the intersection between lines and spheres is used to find the crossing times.\nCrossingAccurateInterpolation(; abstol=1e-12, reltol=1e-6): Extremely accurate high order interpolation is used between integrator steps. First, a minimization with Optim.jl finds the minimum distance of the trajectory to the set center. Then, Roots.jl is used to find the exact crossing point. The tolerances are given to both procedures.\n\nClearly, CrossingAccurateInterpolation is much more accurate than CrossingLinearIntersection, but also much slower. However, the smaller the steps the integrator takes (in case some very high accuracy solver is used), the closer the linear intersection gets to the accurate version. Benchmarks are advised for the individual specific case the algorithm is applied at, in order to choose the best method.\n\nThe keyword threshold_distance = Inf provides a means to skip the interpolation check, if the current state of the integrator is too far from the set center. If the distance of the current state of the integrator is threshold_distance or more distance away from the set center, attempts to interpolate are skipped. By default threshold_distance = Inf and hence this never happens. Typically you'd want this to be 10-100 times the distance the trajectory covers at an average integrator step.\n\n[Meiss1997]: Meiss, J. D. Average exit time for volume-preserving maps, Chaos (1997)\n\n[Boev2014]: Boev, Vadivasova, & Anishchenko, Poincaré recurrence statistics as an indicator of chaos synchronization, Chaos (2014)\n\n\n\n\n\n","category":"function"},{"location":"chaos_detection/#Detecting-and-Categorizing-Chaos","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Being able to detect and distinguish chaotic from regular behavior is crucial in the study of dynamical systems. Most of the time a positive maximum lyapunov exponent and a bounded system indicate chaos.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"However, the convergence of the Lyapunov exponent can be slow, or even misleading, as the types of chaotic behavior vary with respect to their predictability. There are some alternatives, some more efficient and some more accurate in characterizing chaotic and regular motion.","category":"page"},{"location":"chaos_detection/#Generalized-Alignment-Index","page":"Detecting & Categorizing Chaos","title":"Generalized Alignment Index","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"\"GALI\" for sort, is a method that relies on the fact that initially orthogonal deviation vectors tend to align towards the direction of the maximum Lyapunov exponent for chaotic motion. It is one of the most recent and cheapest methods for distinguishing chaotic and regular behavior, introduced first in 2007 by Skokos, Bountis & Antonopoulos.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"gali","category":"page"},{"location":"chaos_detection/#ChaosTools.gali","page":"Detecting & Categorizing Chaos","title":"ChaosTools.gali","text":"gali(ds::DynamicalSystem, T, k::Int; kwargs...) -> GALI_k, t\n\nCompute textGALI_k[Skokos2007] for a given k up to time T. Return textGALI_k(t) and time vector t.\n\nThe third argument sets the order of gali. gali function simply initializes a TangentDynamicalSystem with k deviation vectors and calls the method below. This means that the automatic Jacobian is used by default. Initialize manually a TangentDynamicalSystem if you have a hand-coded Jacobian.\n\nKeyword arguments\n\nthreshold = 1e-12: If GALI_k falls below the threshold iteration is terminated.\nΔt = 1: Time-step between deviation vector normalizations. For continuous systems this is approximate.\nu0: Initial state for the system. Defaults to current_state(ds).\n\nDescription\n\nThe Generalized Alignment Index, textGALI_k, is an efficient (and very fast) indicator of chaotic or regular behavior type in D-dimensional Hamiltonian systems (D is number of variables). The asymptotic behavior of textGALI_k(t) depends critically on the type of orbit resulting from the initial condition. If it is a chaotic orbit, then\n\ntextGALI_k(t) sim\nexpleftsum_j=1^k (lambda_1 - lambda_j)t right\n\nwith lambda_j being the j-th Lyapunov exponent (see lyapunov, lyapunovspectrum). If on the other hand the orbit is regular, corresponding to movement in d-dimensional torus with 1 le d le D2 then it holds\n\ntextGALI_k(t) sim\n    begincases\n      textconst  textif  2 le k le d    textand\n       d  1 \n      t^-(k - d)  textif   d  k le D - d \n      t^-(2k - D)  textif   D - d  k le D\n    endcases\n\nTraditionally, if textGALI_k(t) does not become less than the threshold until T the given orbit is said to be chaotic, otherwise it is regular.\n\nOur implementation is not based on the original paper, but rather in the method described in[Skokos2016b], which uses the product of the singular values of A, a matrix that has as columns the deviation vectors.\n\n[Skokos2007]: Skokos, C. H. et al., Physica D 231, pp 30–54 (2007)\n\n[Skokos2016b]: Skokos, C. H. et al., Chaos Detection and Predictability - Chapter 5 (section 5.3.1 and ref. [85] therein), Lecture Notes in Physics 915, Springer (2016)\n\n\n\n\n\ngali(tands::TangentDynamicalSystem, T; threshold = 1e-12, Δt = 1)\n\nThe low-level method that is called by gali(ds::DynamicalSystem, ...). Use this method for looping over different initial conditions or parameters by calling reinit! to tands.\n\nThe order of textGALI_k computed is the amount of deviation vectors in tands.\n\nAlso use this method if you have a hand-coded Jacobian to pass when creating tands.\n\n\n\n\n\n","category":"function"},{"location":"chaos_detection/#GALI-example","page":"Detecting & Categorizing Chaos","title":"GALI example","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"As an example let's use the Henon-Heiles system","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using ChaosTools, CairoMakie\nusing OrdinaryDiffEq: Vern9\n\nfunction henonheiles_rule(u, p, t)\n    SVector(u[3], u[4],\n        -u[1] - 2u[1]*u[2],\n        -u[2] - (u[1]^2 - u[2]^2),\n    )\nend\nfunction henonheiles_jacob(u, p, t)\n    SMatrix{4,4}(0, 0, -1 - 2u[2], -2u[1], 0, 0,\n     -2u[1], -1 + 2u[2], 1, 0, 0, 0, 0, 1, 0, 0)\nend\n\nu0=[0, -0.25, 0.42081, 0]\nΔt = 1.0\ndiffeq = (abstol=1e-9, retol=1e-9, alg = Vern9(), maxiters = typemax(Int))\nsp = [0, .295456, .407308431, 0] # stable periodic orbit: 1D torus\nqp = [0, .483000, .278980390, 0] # quasiperiodic orbit: 2D torus\nch = [0, -0.25, 0.42081, 0]      # chaotic orbit\nds = CoupledODEs(henonheiles_rule, sp)","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Let's see what happens with a quasi-periodic orbit:","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"tr = trajectory(ds, 10000.0, qp; Δt)[1]\nfig, ax = scatter(tr[:,1], tr[:,3]; label=\"qp\", markersize=2)\naxislegend(ax)\n\nax = Axis(fig[1,2]; yscale = log)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; u0 = qp, Δt)\n    logt = log.(t)\n    lines!(ax, logt, g; label=\"GALI_$(k)\")\n    if k == 2\n        lines!(ax, logt, 1 ./ t.^(2k-4); label=\"slope -$(2k-4)\")\n    else\n        lines!(ax, logt, 100 ./ t.^(2k-4); label=\"slope -$(2k-4)\")\n    end\nend\nylims!(ax, 1e-12, 2)\nfig","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"And here is GALI of a continuous system with a chaotic orbit","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"tr = trajectory(ds, 10000.0, ch; Δt)[1]\nfig, ax = scatter(tr[:,1], tr[:,3]; label=\"ch\", markersize=2, color = (Main.COLORS[1], 0.5))\naxislegend(ax)\n\nax = Axis(fig[1,2]; yscale = log)\nls = lyapunovspectrum(ds, 5000; Δt, u0 = ch)\nfor k in [2,3,4]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = ch, Δt)\n    lines!(t, exp.(-ex.*t); label=\"exp. k=$k\")\n    lines!(t, g; label=\"GALI_$(k)\")\nend\nylims!(ax, 1e-16, 1)\nfig","category":"page"},{"location":"chaos_detection/#Using-GALI","page":"Detecting & Categorizing Chaos","title":"Using GALI","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"No-one in their right mind would try to fit power-laws in order to distinguish between chaotic and regular behavior, like the above examples. These were just proofs that the method works as expected.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The most common usage of textGALI_k is to define a (sufficiently) small amount of time and a (sufficiently) small threshold and see whether textGALI_k stays below it, for a (sufficiently) big k.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"For example, we utilize parallel integration of TangentDynamicalSystem to compute GALI for many initial conditions and produce a color-coded map of regular and chaotic orbits of the standard map.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The following is an example of advanced usage:","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"using ChaosTools, CairoMakie\n# Initialize `TangentDynamicalSystem`\n@inbounds function standardmap_rule(x, par, n)\n    theta = x[1]; p = x[2]\n    p += par[1]*sin(theta)\n    theta += p\n    return mod2pi.(SVector(theta, p))\nend\n@inbounds standardmap_jacob(x, p, n) = SMatrix{2,2}(\n    1 + p[1]*cos(x[1]), p[1]*cos(x[1]), 1, 1\n)\nds = DeterministicIteratedMap(standardmap_rule, ones(2), [1.0])\ntands = TangentDynamicalSystem(ds; J = standardmap_jacob)\n# Collect initial conditions\ndens = 101\nθs = ps = range(0, stop = 2π, length = dens)\nics = vec(SVector{2, Float64}.(Iterators.product(θs, ps)))\n# Initialize as many systems as threads\nsystems = [deepcopy(tands) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, tands)\n# Perform threaded loop\nregularity = zeros(size(ics))\nThreads.@threads for i in eachindex(ics)\n    u0 = ics[i]\n    system = systems[Threads.threadid()]\n    reinit!(system, u0)\n    regularity[i] = gali(system, 500)[2][end]\nend\n# Visualize\nfig, ax, sc = scatter(ics; color = regularity)\nColorbar(fig[1,2], sc; label = \"regularity\")\nfig","category":"page"},{"location":"chaos_detection/#Predictability-of-a-chaotic-system","page":"Detecting & Categorizing Chaos","title":"Predictability of a chaotic system","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Even if a system is \"formally\" chaotic, it can still be in phases where it is partially predictable, because the correlation coefficient between nearby trajectories vanishes very slowly with time. Wernecke, Sándor & Gros have developed an algorithm that allows one to classify a dynamical system to one of three categories: strongly chaotic, partially predictable chaos or regular (called laminar in their paper).","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We have implemented their algorithm in the function predictability. Note that we set up the implementation to always return regular behavior for negative Lyapunov exponent. You may want to override this for research purposes.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"predictability","category":"page"},{"location":"chaos_detection/#ChaosTools.predictability","page":"Detecting & Categorizing Chaos","title":"ChaosTools.predictability","text":"predictability(ds::CoreDynamicalSystem; kwargs...) -> chaos_type, ν, C\n\nDetermine whether ds displays strongly chaotic, partially-predictable chaotic or regular behaviour, using the method by Wernecke et al. described in[Wernecke2017].\n\nReturn the type of the behavior, the cross-distance scaling coefficient ν and the correlation coefficient C. Typical values for ν, C and chaos_type are given in Table 2 of[Wernecke2017]:\n\nchaos_type ν C\n:SC 0 0\n:PPC 0 1\n:REG 1 1\n\nIf none of these conditions apply, the return value is :IND (for indeterminate).\n\nKeyword arguments\n\nTtr = 200: Extra transient time to evolve the system before sampling from  the trajectory. Should be Int for discrete systems.\nT_sample = 1e4: Time to evolve the system for taking samples. Should be Int for discrete systems.\nn_samples = 500: Number of samples to take for use in calculating statistics.\nλ_max = lyapunov(ds, 5000): Value to use for largest Lyapunov exponent for finding the Lyapunov prediction time. If it is less than zero a regular result is returned immediatelly.\nd_tol = 1e-3: tolerance distance to use for calculating Lyapunov prediction time.\nT_multiplier = 10: Multiplier from the Lyapunov prediction time to the evaluation time.\nT_max = Inf: Maximum time at which to evaluate trajectory distance. If the internally  computed evaluation time is larger than T_max, stop at T_max instead.  It is strongly recommended to manually set this!\nδ_range = 10.0 .^ (-9:-6): Range of initial condition perturbation distances  to use to determine scaling ν.\nν_threshold = C_threshold = 0.5: Thresholds for scaling coefficients (they become 0 or 1 if they are less or more than the threshold).\n\nDescription\n\nThe algorithm samples points from a trajectory of the system to be used as initial conditions. Each of these initial conditions is randomly perturbed by a distance δ, and the trajectories for both the original and perturbed initial conditions are evolved up to the 'evaluation time' T (see below its definition).\n\nThe average (over the samples) distance and cross-correlation coefficient of the state at time T is computed. This is repeated for a range of δ (defined by δ_range), and linear regression is used to determine how the distance and cross-correlation scale with δ, allowing for identification of chaos type.\n\nThe evaluation time T is calculated as T = T_multiplier*Tλ, where the Lyapunov prediction time Tλ = log(d_tol/δ)/λ_max. This may be very large if the λ_max is small, e.g. when the system is regular, so this internally computed time T can be overridden by a smaller T_max set by the user.\n\nPerformance Notes\n\nFor continuous systems, it is likely that the maxiters used by the integrators needs to be increased, e.g. to 1e9. This is part of the diffeq kwargs. In addition, be aware that this function does a lot of internal computations. It is operating in a different speed than e.g. lyapunov.\n\n[Wernecke2017]: Wernecke, H., Sándor, B. & Gros, C. How to test for partially predictable chaos. Scientific Reports 7, (2017).\n\n\n\n\n\n","category":"function"},{"location":"chaos_detection/#Example-Hénon-Map","page":"Detecting & Categorizing Chaos","title":"Example Hénon Map","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"We will create something similar to figure 2 of the paper, but for the Hénon map.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"fig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"a\", ylabel = L\"x\")\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhe = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\nas = 0.8:0.01:1.225\nod = orbitdiagram(he, 1, 1, as; n = 2000, Ttr = 2000)\ncolors = Dict(:REG => \"blue\", :PPC => \"green\", :SC => \"red\")\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    chaos_type, ν, C = predictability(he; T_max = 400000, Ttr = 2000)\n    scatter!(ax, a .* ones(length(od[i])), od[i];\n    color = (colors[chaos_type], 0.05), markersize = 2)\nend\nax.title = \"predictability of Hénon map\"\nfig","category":"page"},{"location":"chaos_detection/#The-0-1-test-for-chaos","page":"Detecting & Categorizing Chaos","title":"The 0-1 test for chaos","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The methods mentioned in this page so far require a DynamicalSystem instance. But of course this is not always the case. The so-called \"0 to 1\" test for chaos, by Gottwald & Melbourne, takes as an input a timeseries and outputs a boolean true if the timeseries is chaotic or false if it is not.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"Notice that the method does have a lot of caveats, so you should read the review paper before using. Also, it doesn't work for noisy data.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"testchaos01","category":"page"},{"location":"chaos_detection/#ChaosTools.testchaos01","page":"Detecting & Categorizing Chaos","title":"ChaosTools.testchaos01","text":"testchaos01(x::Vector [, cs, N0]) -> chaotic?\n\nPerform the so called \"0-1\" test for chaos introduced by Gottwald and Melbourne[Gottwald2016] on the timeseries x. Return true if x is chaotic, false otherwise.\n\nDescription\n\nThis method tests if the given timeseries is chaotic or not by transforming it into a two-dimensional diffusive process like so:\n\np_n = sum_j=1^nphi_j cos(j c)quad q_n = sum_j=1^nphi_j sin(j c)\n\nIf the timeseries is chaotic, the mean square displacement of the process grows as sqrt(length(x)), while it stays constant if the timeseries is regular.\n\nThe implementation here computes K, a coefficient measuring the growth of the mean square displacement, and simply checks if K > 0.5. K is the median of K_c over given c, see the reference.\n\nIf you want to access the various Kc you should call the method testchaos01(x, c::Real, N0) which returns Kc. In fact, the high level method is just median(testchaos01(x, c, N0) for c in cs) > 0.5.\n\ncs defaults to 3π/5*rand(100) + π/4 and N0, the length of the two-dimensional process, is N0 = length(x)/10.\n\nFor data sampled from continous dynamical systems, some care must be taken regarding the values of cs. Also note that this method performs rather poorly with even the slight amount of noise, returning true for even small amounts of noise noisy timeseries. Some possibilities to eliviate this exist, but are context specific on the application. See [Gottwald2016] for more info.\n\n[Gottwald2016]: Gottwald & Melbourne, “The 0-1 test for chaos: A review” Lect. Notes Phys., vol. 915, pp. 221–247, 2016.\n\n\n\n\n\n","category":"function"},{"location":"chaos_detection/#Expansion-entropy","page":"Detecting & Categorizing Chaos","title":"Expansion entropy","text":"","category":"section"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"The expansion entropy is a quantity that is suggested by B. Hunt and E. Ott as a measure that can define chaos (so far no widely accepted definition of chaos exists). Positive expansion entropy means chaos.","category":"page"},{"location":"chaos_detection/","page":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","text":"expansionentropy","category":"page"},{"location":"chaos_detection/#ChaosTools.expansionentropy","page":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy","text":"expansionentropy(ds::DynamicalSystem, sampler, isinside; kwargs...)\n\nCalculate the expansion entropy[Hunt2015] of ds, in the restraining region S by estimating the slope (via linear regression) of the curve log E_t0+T t0(f S) versus T (using linear_region). This is an approximation of the expansion entropy H_0, according to[Hunt2015]. Return T,  log E and the calculated slope.\n\nsampler is a 0-argument function that generates a random initial conditions of ds and isinside is a 1-argument function that given a state it returns true if the state is inside the restraining region. Typically sampler, isinside are the output of statespace_sampler.\n\nKeyword arguments\n\nN = 1000: Number of samples taken at each batch (same as N of [Hunt2015]).\nsteps = 40: The maximal steps for which the system will be run.\nbatches = 100: Number of batches to run the calculation, see below.\nΔt = 1: Time evolution step size.\nJ = nothing: Jacobian function given to TangentDynamicalSystem.\n\nDescription\n\nN samples are initialized and propagated forwards in time (along with their tangent space). At every time t in [t0+Δt, t0+2Δt, ..., t0+steps*Δt] we calculate H:\n\nHt = log E_t0+T t0(f S)\n\nwith\n\nE_t0+T t0(f S) = frac 1 N sum_i G(Df_t0+t t0(x_i))\n\n(using same notation as [Hunt2015]). In principle E is the average largest possible growth ratio within the restraining region (sampled by the initial conditions). The summation is only over x_i that stay inside the region S defined by the boolean function insinside. This process is done by the ChaosTools.expansionentropy_sample function.\n\nThen, this is repeated for batches amount of times, as recommended in[Hunt2015]. From all these batches, the mean and std of H is computed at every time point. This is done by the expansionentropy_batch function. When plotted versus t, these create the curves and error bars of e.g. Figs 2, 3 of [1].\n\nThis function expansionentropy simply returns the slope of the biggest linear region of the curve H versus t, which approximates the expansion entropy H_0. It is therefore recommended to use expansionentropy_batch directly and evaluate the result yourself, as this step is known to be inaccurate for non-chaotic systems (where H fluctuates strongly around 0).\n\n[Hunt2015]: Hunt & Ott, ‘Defining Chaos’, Chaos 25.9 (2015)\n\n\n\n\n\n","category":"function"},{"location":"dimreduction/#Dimensionality-reduction","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"","category":"section"},{"location":"dimreduction/#Broomhead-King-Coordinates","page":"Dimensionality reduction","title":"Broomhead-King Coordinates","text":"","category":"section"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"broomhead_king","category":"page"},{"location":"dimreduction/#ChaosTools.broomhead_king","page":"Dimensionality reduction","title":"ChaosTools.broomhead_king","text":"broomhead_king(s::AbstractVector, d::Int) -> U, S, Vtr\n\nReturn the Broomhead-King coordinates of a timeseries s by performing svd on high-dimensional delay embedding if s with dimension d with minimum delay.\n\nDescription\n\nBroomhead and King coordinates is an approach proposed in [Broomhead1987] that applies the Karhunen–Loève theorem to delay coordinates embedding with smallest possible delay.\n\nThe function performs singular value decomposition on the d-dimensional matrix X of s,\n\nX = frac1sqrtNleft(\nbeginarraycccc\nx_1  x_2  ldots  x_d \nx_2  x_3  ldots  x_d+1\nvdots  vdots  vdots  vdots \nx_N-d+1  x_N-d+2 ldots  x_N\nendarray\nright) = Ucdot S cdot V^tr\n\nwhere x = s - bars. The columns of U can then be used as a new coordinate system, and by considering the values of the singular values S you can decide how many columns of U are \"important\".\n\n[Broomhead1987]: Broomhead, Jones, King, J. Phys. A 20, 9, pp L563 (1987)\n\n\n\n\n\n","category":"function"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"This alternative/improvement of the traditional delay coordinates can be a very powerful tool. An example where it shines is noisy data where there is the effect of superficial dimensions due to noise.","category":"page"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"Take the following example where we produce noisy data from a system and then use Broomhead-King coordinates as an alternative to \"vanilla\" delay coordinates:","category":"page"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"using ChaosTools, CairoMakie\n\nfunction gissinger_rule(u, p, t)\n    μ, ν, Γ = p\n    du1 = μ*u[1] - u[2]*u[3]\n    du2 = -ν*u[2] + u[1]*u[3]\n    du3 = Γ - u[3] + u[1]*u[2]\n    return SVector{3}(du1, du2, du3)\nend\n\ngissinger = CoupledODEs(gissinger_rule, ones(3), [0.112, 0.1, 0.9])\nX, t = trajectory(gissinger, 1000.0; Ttr = 100, Δt = 0.1)\nx = X[:, 1]\n\nL = length(x)\ns = x .+ 0.5rand(L) #add noise\n\nU, S = broomhead_king(s, 20)\nsummary(U)","category":"page"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"Now let's simply compare the above result with the one you get from doing a standard delay coordinates embedding","category":"page"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"using DelayEmbeddings: embed, estimate_delay\n\nfig = Figure()\naxs = [Axis3(fig[1, i]) for i in 1:2]\nlines!(axs[1], U[:, 1], U[:, 2], U[:, 3])\naxs[1].title = \"Broomhead-King of s\"\n\nR = embed(s, 3, estimate_delay(x, \"mi_min\"))\nlines!(axs[2], columns(R)...)\naxs[2].title = \"2D embedding of s\"\nfig","category":"page"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"we have used the same system as in the Delay Coordinates Embedding example, and picked the optimal delay time of τ = 30 (for same Δt = 0.05). Regardless, the vanilla delay coordinates is much worse than the Broomhead-King coordinates.","category":"page"},{"location":"dimreduction/#DyCA-Dynamical-Component-Analysis","page":"Dimensionality reduction","title":"DyCA - Dynamical Component Analysis","text":"","category":"section"},{"location":"dimreduction/","page":"Dimensionality reduction","title":"Dimensionality reduction","text":"dyca","category":"page"},{"location":"dimreduction/#ChaosTools.dyca","page":"Dimensionality reduction","title":"ChaosTools.dyca","text":"dyca(data, eig_thresold) -> eigenvalues, proj_mat, projected_data\n\nCompute the Dynamical Component analysis (DyCA) of the given data [Uhl2018] used for dimensionality reduction.\n\nReturn the eigenvalues, projection matrix, and reduced-dimension data (which are just data*proj_mat).\n\nKeyword Arguments\n\nnorm_eigenvectors=false : if true, normalize the eigenvectors\n\nDescription\n\nDynamical Component Analysis (DyCA) is a method to detect projection vectors to reduce the dimensionality of multi-variate, high-dimensional deterministic datasets. Unlike methods like PCA or ICA that make a stochasticity assumption, DyCA relies on a determinacy assumption on the time-series and is based on the solution of a generalized eigenvalue problem. After choosing an appropriate eigenvalue threshold and solving the eigenvalue problem, the obtained eigenvectors are used to project the high-dimensional dataset onto a lower dimension. The obtained eigenvalues measure the quality of the assumption of linear determinism for the investigated data. Furthermore, the number of the generalized eigenvalues with a value of approximately 1.0 are a measure of the number of linear equations contained in the dataset. This property is useful in detecting regions with highly deterministic parts in the time-series and also as a preprocessing step for reservoir computing of high-dimensional spatio-temporal data.\n\nThe generalised eigenvalue problem we solve is:\n\nC_1 C_0^-1 C_1^top baru = lambda C_2 baru\n\n\nwhere C_0 is the correlation matrix of the data with itself, C_1 the correlation matrix of the data with its derivative, and C_2 the correlation matrix of the derivative of the data with itself. The eigenvectors baru with eigenvalues approximately 1 and their C_1^-1 C_2 u counterpart, form the space where the data is projected onto.\n\n[Uhl2018]: B Seifert, K Korn, S Hartmann, C Uhl, Dynamical Component Analysis (DYCA): Dimensionality Reduction for High-Dimensional Deterministic Time-Series, 10.1109/mlsp.2018.8517024, 2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)\n\n\n\n\n\n","category":"function"},{"location":"orbitdiagram/#Orbit-diagrams","page":"Orbit diagrams","title":"Orbit diagrams","text":"","category":"section"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"An orbit diagram is a way to visualize the asymptotic behaviour of a map, when a parameter of the system is changed. In practice an orbit diagram is a simple plot that plots the last n states of a dynamical system at a given parameter, repeated for all parameters in a range of interest. While this concept can apply to any kind of system, it makes most sense in discrete time dynamical systems. See Chapter 4 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022, for a more involved discussion on orbit diagrams for both discrete and continuous time systems.","category":"page"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"orbitdiagram","category":"page"},{"location":"orbitdiagram/#ChaosTools.orbitdiagram","page":"Orbit diagrams","title":"ChaosTools.orbitdiagram","text":"orbitdiagram(ds::DynamicalSystem, i, p_index, pvalues; kwargs...) → od\n\nCompute the orbit diagram (sometimes wrongly called bifurcation diagram) of the given dynamical system, saving the i variable(s) for parameter values pvalues. The p_index specifies which parameter to change via set_parameter!(ds, p_index, pvalue). Works for any kind of DynamicalSystem, although it mostly makes sense with one of DeterministicIteratedMap, StroboscopicMap, PoincareMap.\n\nAn orbit diagram is simply a collection of the last n states of ds as ds is evolved. This is done for each parameter value.\n\ni can be Int or AbstractVector{Int}. If i is Int, od is a vector of vectors. Else od is a vector of vectors of vectors. Each entry od od are the points at each parameter value, so that length(od) == length(pvalues) and length(od[j]) == n, ∀ j.\n\nKeyword arguments\n\nn::Int = 100: Amount of points to save for each parameter value.\nΔt = 1: Stepping time between saving points.\nu0 = nothing: Specify an initial state. If nothing, the previous state after each parameter is used to seed the new initial condition at the new parameter (with the very first state being the system's state). This makes convergence to the attractor faster, necessitating smaller Ttr. Otherwise u0 can be a standard state, or a vector of states, so that a specific state is used for each parameter.\nTtr::Int = 10: Each orbit is evolved for Ttr first before saving output.\nulims = (-Inf, Inf): only record system states within ulims (only valid if i isa Int). Iteration continues until n states fall within ulims.\nshow_progress = false: Display a progress bar (counting the parameter values).\nperiods = nothing: Only valid if ds isa StroboscopicMap. If given, it must be a a container with same layout as pvalues. Provides a value for the period for each parameter value. Useful in case the orbit diagram is produced versus a driving frequency.\n\n\n\n\n\n","category":"function"},{"location":"orbitdiagram/#Deterministic-iterated-map","page":"Orbit diagrams","title":"Deterministic iterated map","text":"","category":"section"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"For example, let's compute the famous orbit diagram of the logistic map:","category":"page"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"using ChaosTools, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nlogistic = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\n\ni = 1\nparameter = 1\npvalues = 2.5:0.004:4\nn = 2000\nTtr = 2000\noutput = orbitdiagram(logistic, i, parameter, pvalues; n, Ttr)\n\nL = length(pvalues)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= pvalues[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfig, ax = scatter(x, y; axis = (xlabel = L\"r\", ylabel = L\"x\"),\n    markersize = 0.8, color = (\"black\", 0.05),\n)\nax.title = \"Logistic map orbit diagram\"\nxlims!(ax, pvalues[1], pvalues[end]); ylims!(ax,0,1)\nfig","category":"page"},{"location":"orbitdiagram/#Stroboscopic-map","page":"Orbit diagrams","title":"Stroboscopic map","text":"","category":"section"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"The beauty of orbitdiagram is that it can be directly applied to any kind of DynamicalSystem. The most useful cases are the already seen DeterministicIteratedMap, but also PoincareMap and StroboscopicMap. Here is an example of the orbit diagram for the Duffing oscillator (making the same as Figure 9.2 of  Nonlinear Dynamics, Datseris & Parlitz, Springer 2022).","category":"page"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"using ChaosTools, CairoMakie\n\nfunction duffing_rule(u,p,t)\n    d, a, ω = p\n    du1 =  u[2]\n    du2 =  -u[1] - u[1]*u[1]*u[1] - d*u[2] + a*sin(ω*t)\n    return SVector(du1, du2)\nend\nT0 = 25.0\np0 = [0.1, 7, 2π/T0]\nu0 = [1.1, 1.1]\nds = CoupledODEs(duffing_rule, u0, p0)\nduffing = StroboscopicMap(ds, T0)\n\n# We want to change both the parameter `ω`, but also the\n# period of the stroboscopic map. `orbitdiagram` allows this!\nTrange = range(8, 26; length = 201)\nωrange = @. 2π / Trange\nn = 200\noutput = orbitdiagram(duffing, 1, 3, ωrange; n, u0, Ttr = 100, periods = Trange)\n\nL = length(Trange)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= Trange[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfig, ax = scatter(x, y; axis = (xlabel = L\"T\", ylabel = L\"u_1\"),\n    markersize = 8, color = (\"blue\", 0.25),\n)\nylims!(ax, -1, 1)\nfig","category":"page"},{"location":"orbitdiagram/","page":"Orbit diagrams","title":"Orbit diagrams","text":"Pro tip: to actually make Fig. 9.2 you'd have to do two modifications: first, pass periods = Trange ./ 2, so that points are recorded every half period. Then, at the very end, do y[2:2:end] .= -y[2:2:end] so that the symmetric orbits are recorded as well","category":"page"},{"location":"#ChaosTools.jl","page":"ChaosTools.jl","title":"ChaosTools.jl","text":"","category":"section"},{"location":"","page":"ChaosTools.jl","title":"ChaosTools.jl","text":"ChaosTools","category":"page"},{"location":"#ChaosTools","page":"ChaosTools.jl","title":"ChaosTools","text":"ChaosTools.jl\n\n(Image: ) (Image: ) (Image: ) (Image: CI) (Image: codecov) (Image: Package Downloads)\n\nA Julia module that offers various tools for analysing nonlinear dynamics and chaotic behaviour. It can be used as a standalone package, or as part of DynamicalSystems.jl.\n\nTo install it, run import Pkg; Pkg.add(\"ChaosTools\").\n\nAll further information is provided in the documentation, which you can either find online or build locally by running the docs/make.jl file.\n\nChaosTools.jl is the jack-of-all-trades package of the DynamicalSystems.jl library: methods that are not extensive enough to be a standalone package are added here. You should see the full DynamicalSystems.jl library for other packages that may contain functionality you are looking for but did not find in ChaosTools.jl.\n\n\n\n\n\n","category":"module"},{"location":"","page":"ChaosTools.jl","title":"ChaosTools.jl","text":"note: Accompanying textbook\nA good background for understanding the methods of ChaosTools.jl is the following textbook: Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.","category":"page"},{"location":"#DynamicalSystemsBase.jl-reference","page":"ChaosTools.jl","title":"DynamicalSystemsBase.jl reference","text":"","category":"section"},{"location":"","page":"ChaosTools.jl","title":"ChaosTools.jl","text":"As many docstrings in ChaosTools.jl point to the different DynamicalSystem types, they are also provided here for reference.","category":"page"},{"location":"","page":"ChaosTools.jl","title":"ChaosTools.jl","text":"DynamicalSystem\nDeterministicIteratedMap\nCoupledODEs\nCoreDynamicalSystem\nStroboscopicMap\nPoincareMap\nTangentDynamicalSystem\nParallelDynamicalSystem\nProjectedDynamicalSystem\nreinit!","category":"page"},{"location":"","page":"ChaosTools.jl","title":"ChaosTools.jl","text":"DynamicalSystem\nDeterministicIteratedMap\nCoupledODEs\nCoreDynamicalSystem\nStroboscopicMap\nPoincareMap\nTangentDynamicalSystem\nParallelDynamicalSystem\nProjectedDynamicalSystem\nreinit!(::DynamicalSystem, args...; kwargs...)","category":"page"},{"location":"#DynamicalSystemsBase.DynamicalSystem","page":"ChaosTools.jl","title":"DynamicalSystemsBase.DynamicalSystem","text":"DynamicalSystem\n\nDynamicalSystem is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library.\n\nAll concrete implementations of DynamicalSystem can be iteratively evolved in time via the step! function. Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has on for parallelization.\n\nDynamicalSystem is further separated into two abstract types: ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem. The simplest and most common concrete implementations of a DynamicalSystem are DeterministicIteratedMap or CoupledODEs.\n\nDescription\n\nnote: Note\nThe documentation of DynamicalSystem follows chapter 1 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022.\n\nA ds::DynamicalSystem representes a flow Φ in a state space. It mainly encapsulates three things:\n\nA state, typically referred to as u, with initial value u0. The space that u occupies is the state space of ds and the length of u is the dimension of ds (and of the state space).\nA dynamic rule, typically referred to as f, that dictates how the state evolves/changes with time when calling the step! function. f is a standard Julia function, see below.\nA parameter container p that parameterizes f. p can be anything, but in general it is recommended to be a type-stable mutable container.\n\nIn sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of DynamicalSystem are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items.\n\nIn this scope dynamical systems have a known dynamic rule f defined as a standard Julia function. Observed or measured data from a dynamical system are represented using StateSpaceSet and are finite. Such data are obtained from the trajectory function or from an experimental measurement of a dynamical system with an unknown dynamic rule.\n\nConstruction instructions on f and u\n\nMost of the concrete implementations of DynamicalSystem, with the exception of ArbitrarySteppable, have two ways of implementing the dynamic rule f, and as a consequence the type of the state u. The distinction is done on whether f is defined as an in-place (iip) function or out-of-place (oop) function.\n\noop : f must be in the form f(u, p, t) -> out   which means that given a state u::SVector{<:Real} and some parameter container   p it returns the output of f as an SVector{<:Real} (static vector).\niip : f must be in the form f!(out, u, p, t)   which means that given a state u::AbstractArray{<:Real} and some parameter container p,   it writes in-place the output of f in out::AbstractArray{<:Real}.   The function must return nothing as a final statement.\n\nt stands for current time in both cases. iip is suggested for systems with high dimension and oop for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of f.\n\nnote: Autonomous vs non-autonomous systems\nWhether the dynamical system is autonomous (f doesn't depend on time) or not, it is still necessary to include t as an argument to f. Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way. You can also convert any system to autonomous by making time an additional variable. If the system is non-autonomous, its effective dimensionality is dimension(ds)+1.\n\nAPI\n\nThe API that the interface of DynamicalSystem employs is the functions listed below. Once a concrete instance of a subtype of DynamicalSystem is obtained, it can quieried or altered with the following functions.\n\nThe main use of a concrete dynamical system instance is to provide it to downstream functions such as lyapunovspectrum from ChaosTools.jl or basins_of_attraction from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems.\n\nAPI - information\n\nds(t) with ds an instance of DynamicalSystem: return the state of ds at time t. For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if t is the current time.\ncurrent_state\ninitial_state\ncurrent_parameters\ninitial_parameters\nisdeterministic\nisdiscretetime\ndynamic_rule\ncurrent_time\ninitial_time\nisinplace\n\nAPI - alter status\n\nreinit!\nset_state!\nset_parameter!\nset_parameters!\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.DeterministicIteratedMap","page":"ChaosTools.jl","title":"DynamicalSystemsBase.DeterministicIteratedMap","text":"DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0)\n\nA deterministic discrete time dynamical system defined by an iterated map as follows:\n\nvecu_n+1 = vecf(vecu_n p n)\n\nAn alias for DeterministicIteratedMap is DiscreteDynamicalSystem.\n\nOptionally configure the parameter container p and initial time t0.\n\nFor construction instructions regarding f, u0 see DynamicalSystem.\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.CoupledODEs","page":"ChaosTools.jl","title":"DynamicalSystemsBase.CoupledODEs","text":"CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0)\n\nA deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows:\n\nfracdvecudt = vecf(vecu p t)\n\nAn alias for CoupledODE is ContinuousDynamicalSystem.\n\nOptionally provide the parameter container p and initial time as keyword t0.\n\nFor construction instructions regarding f, u0 see DynamicalSystem.\n\nDifferentialEquations.jl keyword arguments and interfacing\n\nThe ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a CoupledODEs, you can specify the solver that will integrate f in time, along with any other integration options, using the diffeq keyword. For example you could use diffeq = (abstol = 1e-9, reltol = 1e-9). If you want to specify a solver, do so by using the keyword alg, e.g.: diffeq = (alg = Tsit5(), reltol = 1e-6). This requires you to have been first using OrdinaryDiffEq to access the solvers. The default diffeq is:\n\n(alg = Tsit5(stagelimiter! = triviallimiter!, steplimiter! = triviallimiter!, thread = static(false)), abstol = 1.0e-6, reltol = 1.0e-6)\n\ndiffeq keywords can also include callback for event handling , however the majority of downstream functions in DynamicalSystems.jl assume that f is differentiable.\n\nThe convenience constructor CoupledODEs(prob::ODEProblem, diffeq) and CoupledODEs(ds::CoupledODEs, diffeq) are also available.\n\nDev note: CoupledODEs is a light wrapper of ODEIntegrator from DifferentialEquations.jl. The integrator is available as the field integ, and the ODEProblem is integ.sol.prob. The convenience syntax ODEProblem(ds::CoupledODEs, tspan = (t0, Inf)) is available.\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.CoreDynamicalSystem","page":"ChaosTools.jl","title":"DynamicalSystemsBase.CoreDynamicalSystem","text":"CoreDynamicalSystem\n\nUnion type meaning either DeterministicIteratedMap or CoupledODEs, which are the core systems whose dynamic rule f is known analytically.\n\nThis type is used for deciding whether a creation of a TangentDynamicalSystem is possible or not.\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.StroboscopicMap","page":"ChaosTools.jl","title":"DynamicalSystemsBase.StroboscopicMap","text":"StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...)\n\nA discrete time dynamical system that produces iterations of a time-dependent (non-autonomous) CoupledODEs system exactly over a given period. The second signature first creates a CoupledODEs and then calls the first.\n\nStroboscopicMap follows the DynamicalSystem interface. In addition, the function set_period!(smap, period) is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time, current_time and initial_time are integers. The initial time is always 0, because current_time counts elapsed periods. Call these functions on the parent of StroboscopicMap to obtain the corresponding continuous time. In contrast, reinit! expects t0 in continuous time.\n\nThe convenience constructor\n\nStroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap\n\nis also provided.\n\nSee also PoincareMap.\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.PoincareMap","page":"ChaosTools.jl","title":"DynamicalSystemsBase.PoincareMap","text":"PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap\n\nA discrete time dynamical system that produces iterations over the Poincaré map[DatserisParlitz2022] of the given continuous time ds. This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the plane argument.\n\nSee also StroboscopicMap, poincaresos.\n\nKeyword arguments\n\ndirection = -1: Only crossings with sign(direction) are considered to belong to the surface of section. Positive direction means going from less than b to greater than b.\nu0 = nothing: Specify an initial state.\nrootkw = (xrtol = 1e-6, atol = 1e-8): A NamedTuple of keyword arguments passed to find_zero from Roots.jl.\nTmax = 1e3: The argument Tmax exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one step! the system has been evolved for more than Tmax, then step!(pmap) will terminate and error.\n\nDescription\n\nThe Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane. PoincareMap iterates over the crossings of the section.\n\nIf the state of ds is mathbfu = (u_1 ldots u_D) then the equation defining a hyperplane is\n\na_1u_1 + dots + a_Du_D = mathbfacdotmathbfu=b\n\nwhere mathbfa b are the parameters of the hyperplane.\n\nIn code, plane can be either:\n\nA Tuple{Int, <: Real}, like (j, r): the plane is defined as when the jth variable of the system equals the value r.\nA vector of length D+1. The first D elements of the vector correspond to mathbfa while the last element is b.\n\nPoincareMap uses ds, higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section.\n\nPoincareMap follows the DynamicalSystem interface with the following adjustments:\n\ndimension(pmap) == dimension(ds), even though the Poincaré map is effectively 1 dimension less.\nLike StroboscopicMap time is discrete and counts the iterations on the surface of section. initial_time is always 0 and current_time is current iteration number.\nA new function current_crossing_time returns the real time corresponding to the latest crossing of the hyperplane, which is what the current_state(ds) corresponds to as well.\nFor the special case of plane being a Tuple{Int, <:Real}, a special reinit! method is allowed with input state of length D-1 instead of D, i.e., a reduced state already on the hyperplane that is then converted into the D dimensional state.\n\nExample\n\nusing DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap)\n\n[DatserisParlitz2022]: Datseris & Parlitz 2022, Nonlinear Dynamics: A Concise Introduction Interlaced with Code, Springer Nature, Undergrad. Lect. Notes In Physics\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.TangentDynamicalSystem","page":"ChaosTools.jl","title":"DynamicalSystemsBase.TangentDynamicalSystem","text":"TangentDynamicalSystem <: DynamicalSystem\nTangentDynamicalSystem(ds::CoreDynamicalSystem; kwargs...)\n\nA dynamical system that bundles the evolution of ds (which must be an CoreDynamicalSystem) and k deviation vectors that are evolved according to the dynamics in the tangent space (also called linearized dynamics or the tangent dynamics).\n\nThe state of ds must be an AbstractVector for TangentDynamicalSystem.\n\nTangentDynamicalSystem follows the DynamicalSystem interface with the following adjustments:\n\nreinit! takes an additional keyword Q0 (with same default as below)\nThe additional functions current_deviations and set_deviations! are provided for the deviation vectors.\n\nKeyword arguments\n\nk or Q0: Q0 represents the initial deviation vectors (each column = 1 vector). If k::Int is given, a matrix Q0 is created with the first k columns of the identity matrix. Otherwise Q0 can be given directly as a matrix. It must hold that size(Q, 1) == dimension(ds). You can use orthonormal for random orthonormal vectors. By default k = dimension(ds) is used.\nu0 = current_state(ds): Starting state.\nJ and J0: See section \"Jacobian\" below.\n\nDescription\n\nLet u be the state of ds, and y a deviation (or perturbation) vector. These two are evolved in parallel according to\n\nbeginarrayrcl\nfracdvecxdt = f(vecx) \nfracdYdt = J_f(vecx) cdot Y\nendarray\nquad mathrmorquad\nbeginarrayrcl\nvecx_n+1 = f(vecx_n) \nY_n+1 = J_f(vecx_n) cdot Y_n\nendarray\n\nfor continuous or discrete time respectively. Here f is the dynamic_rule(ds) and J_f is the Jacobian of f.\n\nJacobian\n\nThe keyword J provides the Jacobian function. It must be a Julia function in the same form as f, the dynamic_rule. Specifically, J(u, p, n) -> M::SMatrix for the out-of-place version or J(M, u, p, n) for the in-place version acting in-place on M. in both cases M is a matrix whose columns are the deviation vectors.\n\nBy default J = nothing.  In this case J is constructed automatically using the module ForwardDiff, hence its limitations also apply here. Even though ForwardDiff is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so it is recommended. Additionally, automatic and in-place Jacobians cannot be time dependent.\n\nThe keyword J0 allows you to pass an initialized Jacobian matrix J0. This is useful for large in-place systems where only a few components of the Jacobian change during the time evolution. J0 can be a sparse or any other matrix type. If not given, a matrix of zeros is used. J0 is ignored for out of place systems.\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.ParallelDynamicalSystem","page":"ChaosTools.jl","title":"DynamicalSystemsBase.ParallelDynamicalSystem","text":"ParallelDynamicalSystem <: DynamicalSystem\nParallelDynamicalSystem(ds::DynamicalSystem, states::Vector{<:AbstractArray})\n\nA struct that evolves several states of a given dynamical system in parallel at exactly the same times. Useful when wanting to evolve several different trajectories of the same system while ensuring that they share parameters and time vector.\n\nThis struct follows the DynamicalSystem interface with the following adjustments:\n\nThe function current_state is called as current_state(pds, i::Int = 1) which returns the ith state. Same for initial_state.\nSimilarly, set_state! obtains a second argument i::Int = 1 to set the i-th state.\ncurrent_states and initial_states can be used to get all parallel states.\nreinit! takes in a vector of states (like states) for u.\n\n\n\n\n\n","category":"type"},{"location":"#DynamicalSystemsBase.ProjectedDynamicalSystem","page":"ChaosTools.jl","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","text":"ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state)\n\nA dynamical system that represents a projection of an existing ds on a (projected) space.\n\nThe projection defines the projected space. If projection isa AbstractVector{Int}, then the projected space is simply the variable indices that projection contains. Otherwise, projection can be an arbitrary function that given the state of the original system ds, returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original.\n\ncomplete_state produces the state for the original system from the projected state. complete_state can always be a function that given the projected state returns a state in the original space. However, if projection isa AbstractVector{Int}, then complete_state can also be a vector that contains the values of the remaining variables of the system, i.e., those not contained in the projected space. In this case the projected space needs to be lower-dimensional than the original.\n\nNotice that ProjectedDynamicalSystem does not require an invertible projection, complete_state is only used during reinit!. ProjectedDynamicalSystem is in fact a rather trivial wrapper of ds which steps it as normal in the original state space and only projects as a last step, e.g., during current_state.\n\nExamples\n\nCase 1: project 5-dimensional system to its last two dimensions.\n\nds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\npds = projected_integrator(ds, projection, complete_state)\nreinit!(pds, [0.2, 0.4])\nstep!(pds)\nget_state(pds)\n\nCase 2: custom projection to general functions of state. julia ds = Systems.lorenz96(5) projection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)] complete_state(y) = repeat(y[1]/5, 5) pds = # same as in above example...`\n\n\n\n\n\n","category":"type"},{"location":"#SciMLBase.reinit!-Tuple{DynamicalSystem, Vararg{Any}}","page":"ChaosTools.jl","title":"SciMLBase.reinit!","text":"reinit!(ds::DynamicalSystem, u = initial_state(ds); kwargs...) → ds\n\nReset the status of ds, so that it is as if it has be just initialized with initial state u. Practically every function of the ecosystem that evolves ds first calls this function on it. Besides the new initial state u, you can also configure the keywords t0 = initial_time(ds) and p = current_parameters(ds).\n\nNote the default settings: the state and time are the initial, but the parameters are the current.\n\nThe special method reinit!(ds, ::Nothing; kwargs...) is also available, which does nothing and leaves the system as is. This is so that downstream functions that call reinit! can still be used without resetting the system but rather continuing from its exact current state.\n\n\n\n\n\n","category":"method"},{"location":"lyapunovs/#Lyapunov-Exponents","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"","category":"section"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Lyapunov exponents measure exponential rates of separation of nearby trajectories in the flow of a dynamical system. The concept of these exponents is best explained in Chapter 3 of Nonlinear Dynamics, Datseris & Parlitz, Springer 2022. The explanations of the chapter directly utilize the code of the functions in this page.","category":"page"},{"location":"lyapunovs/#Lyapunov-Spectrum","page":"Lyapunov Exponents","title":"Lyapunov Spectrum","text":"","category":"section"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"The function lyapunovspectrum calculates the entire spectrum of the Lyapunov exponents of a system:","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovspectrum","category":"page"},{"location":"lyapunovs/#ChaosTools.lyapunovspectrum","page":"Lyapunov Exponents","title":"ChaosTools.lyapunovspectrum","text":"lyapunovspectrum(ds::DynamicalSystem, N, k = dimension(ds); kwargs...) -> λs\n\nCalculate the spectrum of Lyapunov exponents [Lyapunov1992] of ds by applying a QR-decomposition on the parallelepiped defined by the deviation vectors, in total for N evolution steps. Return the spectrum sorted from maximum to minimum. The third argument k is optional, and dictates how many lyapunov exponents to calculate (defaults to dimension(ds)).\n\nSee also lyapunov, local_growth_rates.\n\nNote: This function simply initializes a TangentDynamicalSystem and calls the method below. This means that the automatic Jacobian is used by default. Initialize manually a TangentDynamicalSystem if you have a hand-coded Jacobian.\n\nKeyword arguments\n\nu0 = current_state(ds): State to start from.\nTtr = 0: Extra transient time to evolve the system before application of the algorithm. Should be Int for discrete systems. Both the system and the deviation vectors are evolved for this time.\nΔt = 1: Time of individual evolutions between successive orthonormalization steps. For continuous systems this is approximate.\nshow_progress = false: Display a progress bar of the process.\n\nDescription\n\nThe method we employ is \"H2\" of [Geist1990], originally stated in [Benettin1980], and explained in educational form in [DatserisParlitz2022].\n\nThe deviation vectors defining a D-dimensional parallepiped in tangent space are evolved using the tangent dynamics of the system (see TangentDynamicalSystem). A QR-decomposition at each step yields the local growth rate for each dimension of the parallepiped. At each step the parallepiped is re-normalized to be orthonormal. The growth rates are then averaged over N successive steps, yielding the lyapunov exponent spectrum.\n\n[Lyapunov1992]: A. M. Lyapunov, The General Problem of the Stability of Motion, Taylor & Francis (1992)\n\n[Geist1990]: K. Geist et al., Progr. Theor. Phys. 83, pp 875 (1990)\n\n[Benettin1980]: G. Benettin et al., Meccanica 15, pp 9-20 & 21-30 (1980)\n\n[DatserisParlitz2022]: Datseris & Parlitz 2022, Nonlinear Dynamics: A Concise Introduction Interlaced with Code, Springer Nature, Undergrad. Lect. Notes In Physics\n\n\n\n\n\nlyapunovspectrum(tands::TangentDynamicalSystem, N::Int; Ttr, Δt, show_progress)\n\nThe low-level method that is called by lyapunovspectrum(ds::DynamicalSystem, ...). Use this method for looping over different initial conditions or parameters by calling reinit! to tands.\n\nAlso use this method if you have a hand-coded Jacobian to pass when creating tands.\n\n\n\n\n\n","category":"function"},{"location":"lyapunovs/#Example","page":"Lyapunov Exponents","title":"Example","text":"","category":"section"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"For example, the Lyapunov spectrum of the folded towel map is calculated as:","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using ChaosTools\nfunction towel_rule(x, p, n)\n    @inbounds x1, x2, x3 = x[1], x[2], x[3]\n    SVector( 3.8*x1*(1-x1) - 0.05*(x2+0.35)*(1-2*x3),\n    0.1*( (x2+0.35)*(1-2*x3) - 1 )*(1 - 1.9*x1),\n    3.78*x3*(1-x3)+0.2*x2 )\nend\nfunction towel_jacob(x, p, n)\n    row1 = SVector(3.8*(1 - 2x[1]), -0.05*(1-2x[3]), 0.1*(x[2] + 0.35))\n    row2 = SVector(-0.19((x[2] + 0.35)*(1-2x[3]) - 1),  0.1*(1-2x[3])*(1-1.9x[1]),  -0.2*(x[2] + 0.35)*(1-1.9x[1]))\n    row3 = SVector(0.0,  0.2,  3.78(1-2x[3]))\n    return vcat(row1', row2', row3')\nend\n\nds = DeterministicIteratedMap(towel_rule, [0.085, -0.121, 0.075], nothing)\ntands = TangentDynamicalSystem(ds; J = towel_jacob)\n\nλλ = lyapunovspectrum(tands, 10000)","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovspectrum also works for continuous time systems and will auto-generate a Jacobian function if one is not give. For example,","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"function lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nlor = CoupledODEs(lorenz_rule, fill(10.0, 3), [10, 32, 8/3])\nλλ = lyapunovspectrum(lor, 10000; Δt = 0.1)","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunovspectrum is also very fast:","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using BenchmarkTools\nds = DeterministicIteratedMap(towel_rule, [0.085, -0.121, 0.075], nothing)\ntands = TangentDynamicalSystem(ds; J = towel_jacob)\n\n@btime lyapunovspectrum($tands, 10000)","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"  966.500 μs (10 allocations: 576 bytes) # on my laptop","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Here is an example of using reinit! to efficiently iterate over different parameter values, and parallelize via Threads, to compute the exponents over a given parameter range.","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using ChaosTools, CairoMakie\n\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\nds = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\ntands = TangentDynamicalSystem(ds; J = henon_jacob)\n\nas = 0.8:0.005:1.225;\nλs = zeros(length(as), 2)\n\n# Since `DynamicalSystem`s are mutable, we need to copy to parallelize\nsystems = [deepcopy(tands) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, tands)\n\nThreads.@threads for i in eachindex(as)\n    system = systems[Threads.threadid()]\n    set_parameter!(system, 1, as[i])\n    λs[i, :] .= lyapunovspectrum(system, 10000; Ttr = 500)\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"a\", ylabel = L\"\\lambda\")\nfor j in 1:2\n    lines!(ax, as, λs[:, j])\nend\nfig","category":"page"},{"location":"lyapunovs/#Maximum-Lyapunov-Exponent","page":"Lyapunov Exponents","title":"Maximum Lyapunov Exponent","text":"","category":"section"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"It is possible to get only the maximum Lyapunov exponent simply by giving 1 as the third argument of lyapunovspectrum. However, there is a second algorithm that calculates the maximum exponent:","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunov","category":"page"},{"location":"lyapunovs/#ChaosTools.lyapunov","page":"Lyapunov Exponents","title":"ChaosTools.lyapunov","text":"lyapunov(ds::DynamicalSystem, Τ; kwargs...) -> λ\n\nCalculate the maximum Lyapunov exponent λ using a method due to Benettin [Benettin1976], which simply evolves two neighboring trajectories (one called \"given\" and one called \"test\") while constantly rescaling the test one.\n\nT  denotes the total time of evolution (should be Int for discrete time systems).\n\nSee also lyapunovspectrum, local_growth_rates.\n\nKeyword arguments\n\nshow_progress = false: Display a progress bar of the process.\nu0 = initial_state(ds): Initial condition.\nTtr = 0: Extra \"transient\" time to evolve the trajectories before starting to measure the expontent. Should be Int for discrete systems.\nd0 = 1e-9: Initial & rescaling distance between the two neighboring trajectories.\nd0_lower = 1e-3*d0: Lower distance threshold for rescaling.\nd0_upper = 1e+3*d0: Upper distance threshold for rescaling.\nΔt = 1: Time of evolution between each check rescaling of distance. For continuous time systems this is approximate.\ninittest = (u1, d0) -> u1 .+ d0/sqrt(length(u1)): A function that given (u1, d0) initializes the test state with distance d0 from the given state u1  (D is the dimension of the system). This function can be used when you want to avoid the test state appearing in a region of the phase-space where it would have e.g. different energy or escape to infinity.\n\nDescription\n\nTwo neighboring trajectories with initial distance d0 are evolved in time. At time t_i if their distance d(t_i) either exceeds the d0_upper, or is lower than d0_lower, the test trajectory is rescaled back to having distance d0 from the reference one, while the rescaling keeps the difference vector along the maximal expansion/contraction direction: u_2 to u_1+(u_2u_1)(d(t_i)d_0).\n\nThe maximum Lyapunov exponent is the average of the time-local Lyapunov exponents\n\nlambda = frac1t_n - t_0sum_i=1^n\nlnleft( a_i right)quad a_i = fracd(t_i)d_0\n\nPerformance notes\n\nThis function simply initializes a ParallelDynamicalSystem and calls the method below.\n\n[Benettin1976]: G. Benettin et al., Phys. Rev. A 14, pp 2338 (1976)\n\n\n\n\n\nlyapunov(pds::ParallelDynamicalSystem, T; Ttr, Δt, d0, d0_upper, d0_lower)\n\nThe low-level method that is called by lyapunov(ds::DynamicalSystem, ...). Use this method for looping over different initial conditions or parameters by calling reinit! to pds.\n\n\n\n\n\n","category":"function"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"For example:","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using ChaosTools\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\nλ = lyapunov(henon, 10000; d0 = 1e-7, d0_upper = 1e-4, Ttr = 100)","category":"page"},{"location":"lyapunovs/#Local-Growth-Rates","page":"Lyapunov Exponents","title":"Local Growth Rates","text":"","category":"section"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"local_growth_rates","category":"page"},{"location":"lyapunovs/#ChaosTools.local_growth_rates","page":"Lyapunov Exponents","title":"ChaosTools.local_growth_rates","text":"local_growth_rates(ds::DynamicalSystem, points::Dataset; kwargs...) → λlocal\n\nCompute the local exponential growth rate(s) of perturbations of the dynamical system ds for initial conditions given in points. For each initial condition u ∈ points, S total perturbations are created and evolved exactly for time Δt. The exponential local growth rate is defined simply by log(g/g0)/Δt with g0 the initial pertrubation size and g the size after Δt. Thus, λlocal is a matrix of size (length(points), S).\n\nThis function is a modification of lyapunov. It uses the full nonlinear dynamics and a ParallelDynamicalSystem to evolve the perturbations, but does not do any re-scaling, thus allowing probing state and time dependence of perturbation growth. The actual growth is given by exp(λlocal * Δt).\n\nThe output of this function is sometimes called \"Nonlinear Local Lyapunov Exponent\".\n\nKeyword arguments\n\nS = 100\nΔt = 5\nperturbation: If given, it should be a function perturbation(ds, u, j) that outputs a pertrubation vector (preferrably SVector) given the system, current initial condition u and the counter j ∈ 1:S. If not given, a random perturbation is generated with norm given by the keyword e = 1e-6.\n\n\n\n\n\n","category":"function"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Here is a simple example using the Henon map","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using ChaosTools\nusing Statistics, CairoMakie\n\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhe = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\npoints = trajectory(he, 2000; Ttr = 100)[1]\n\nλlocal = local_growth_rates(he, points; Δt = 1)\n\nλmeans = mean(λlocal; dims = 2)\nλstds = std(λlocal; dims = 2)\nx, y = columns(points)\nfig, ax, obj = scatter(x, y; color = vec(λmeans))\nColorbar(fig[1,2], obj)\nfig","category":"page"},{"location":"lyapunovs/#Lyapunov-exponent-from-data","page":"Lyapunov Exponents","title":"Lyapunov exponent from data","text":"","category":"section"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"lyapunov_from_data\nNeighborNumber\nWithinRange","category":"page"},{"location":"lyapunovs/#ChaosTools.lyapunov_from_data","page":"Lyapunov Exponents","title":"ChaosTools.lyapunov_from_data","text":"lyapunov_from_data(R::Dataset, ks; kwargs...)\n\nFor the given dataset R, which is expected to represent a trajectory of a dynamical system, calculate and return E(k), which is the average logarithmic distance between states of a neighborhood that are evolved in time for k steps (k must be integer). The slope of E vs k approximates the maximum Lyapunov exponent.\n\nTypically R is the result of delay coordinates embedding of a timeseries (see DelayEmbeddings.jl).\n\nKeyword arguments\n\nrefstates = 1:(length(R) - ks[end]): Vector of indices that notes which states of the dataset should be used as \"reference states\", which means that the algorithm is applied for all state indices contained in refstates.\nw::Int = 1: The Theiler window.\nntype = NeighborNumber(1): The neighborhood type. Either NeighborNumber or WithinRange. See Neighborhoods for more info.\ndistance = FirstElement(): Specifies what kind of distance function is used in the logarithmic distance of nearby states. Allowed distances values are FirstElement() or Euclidean(), see below for more info. The metric for finding neighbors is always the Euclidean one.\n\nDescription\n\nIf the dataset exhibits exponential divergence of nearby states, then it should hold\n\nE(k) approx lambdacdot k cdot Delta t + E(0)\n\nfor a well defined region in the k axis, where lambda is the approximated maximum Lyapunov exponent. Delta t is the time between samples in the original timeseries. You can use linear_region with arguments (ks .* Δt, E) to identify the slope (= lambda) immediatelly, assuming you have choosen sufficiently good ks such that the linear scaling region is bigger than the saturated region.\n\nThe algorithm used in this function is due to Parlitz[Skokos2016], which itself expands upon Kantz[Kantz1994]. In sort, for each reference state a neighborhood is evaluated. Then, for each point in this neighborhood, the logarithmic distance between reference state and neighborhood state(s) is calculated as the \"time\" index k increases. The average of the above over all neighborhood states over all reference states is the returned result.\n\nIf the distance is Euclidean() then use the Euclidean distance of the full D-dimensional points (distance d_E in ref.[Skokos2016]). If however the distance is FirstElement(), calculate the absolute distance of only the first elements of the points of R (distance d_F in ref.[Skokos2016], useful when R comes from delay embedding).\n\n[Skokos2016]: Skokos, C. H. et al., Chaos Detection and Predictability - Chapter 1 (section 1.3.2), Lecture Notes in Physics 915, Springer (2016)\n\n[Kantz1994]: Kantz, H., Phys. Lett. A 185, pp 77–87 (1994)\n\n\n\n\n\n","category":"function"},{"location":"lyapunovs/#Neighborhood.NeighborNumber","page":"Lyapunov Exponents","title":"Neighborhood.NeighborNumber","text":"NeighborNumber(k::Int) <: SearchType\n\nSearch type representing the k nearest neighbors of the query (or approximate neighbors, depending on the search structure).\n\n\n\n\n\n","category":"type"},{"location":"lyapunovs/#Neighborhood.WithinRange","page":"Lyapunov Exponents","title":"Neighborhood.WithinRange","text":"WithinRange(r::Real) <: SearchType\n\nSearch type representing all neighbors with distance ≤ r from the query (according to the search structure's metric).\n\n\n\n\n\n","category":"type"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Let's apply the method to a timeseries from a continuous time system. In this case, one must be a bit more thoughtful when choosing parameters. The following example helps the users get familiar with the process:","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using ChaosTools, CairoMakie\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nds = CoupledODEs(lorenz_rule, fill(10.0, 3), [10, 32, 8/3])\n# create a timeseries of 1 dimension\nΔt = 0.05\nx = trajectory(ds, 1000.0; Ttr = 10, Δt)[1][:, 1]","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"From prior knowledge of the system, we know we need to use k up to about 150. However, due to the dense time sampling, we don't have to compute for every k in the range 0:150. Instead, we can use","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"ks = 0:4:150","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"Now we plot some example computations using delay embeddings to \"reconstruct\" the chaotic attractor","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"using DelayEmbeddings: embed\nfig = Figure()\nax = Axis(fig[1,1]; xlabel=\"k (0.05×t)\", ylabel=\"E - E(0)\")\nntype = NeighborNumber(5) #5 nearest neighbors of each state\n\nfor d in [4, 8], τ in [7, 15]\n    r = embed(x, d, τ)\n\n    # E1 = lyapunov_from_data(r, ks1; ntype)\n    # λ1 = ChaosTools.linreg(ks1 .* Δt, E1)[2]\n    # plot(ks1,E1.-E1[1], label = \"dense, d=$(d), τ=$(τ), λ=$(round(λ1, 3))\")\n\n    E2 = lyapunov_from_data(r, ks; ntype)\n    λ2 = ChaosTools.linreg(ks .* Δt, E2)[2]\n    lines!(ks, E2.-E2[1]; label = \"d=$(d), τ=$(τ), λ=$(round(λ2, digits = 3))\")\nend\naxislegend(ax; position = :lt)\nax.title = \"Continuous Reconstruction Lyapunov\"\nfig","category":"page"},{"location":"lyapunovs/","page":"Lyapunov Exponents","title":"Lyapunov Exponents","text":"As you can see, using τ = 15 is not a great choice! The estimates with τ = 7 though are very good (the actual value is around λ ≈ 0.89...). Notice that above a linear regression was done over the whole curves, which doesn't make sense. One should identify a linear scaling region and extract the slope of that one. The function linear_region from FractalDimensions.jl does this!","category":"page"},{"location":"periodicity/#Fixed-points-and-Periodicity","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"","category":"section"},{"location":"periodicity/#Fixed-points","page":"Fixed points & Periodicity","title":"Fixed points","text":"","category":"section"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"fixedpoints","category":"page"},{"location":"periodicity/#ChaosTools.fixedpoints","page":"Fixed points & Periodicity","title":"ChaosTools.fixedpoints","text":"fixedpoints(ds::CoreDynamicalSystem, box, J = nothing; kwargs...) → fp, eigs, stable\n\nReturn all fixed points fp of the given out-of-place ds (either DeterministicIteratedMap or CoupledODEs) that exist within the state space subset box for parameter configuration p. Fixed points are returned as a Dataset. For convenience, a vector of the Jacobian eigenvalues of each fixed point, and whether the fixed points are stable or not, are also returned.\n\nbox is an appropriate IntervalBox from IntervalRootFinding.jl. E.g. for a 3D system it would be something like\n\nv, z = -5..5, -2..2   # 1D intervals, can use `interval(-5, 5)` instead\nbox = v × v × z       # `\\times = ×`, or use `IntervalBox(v, v, z)` instead\n\nJ is the Jacobian of the dynamic rule of ds. It is like in TangentDynamicalSystem, however in this case automatic Jacobian estimation does not work, hence a hand-coded version must be given.\n\nInternally IntervalRootFinding.jl is used and as a result we are guaranteed to find all fixed points that exist in box, regardless of stability. Since IntervalRootFinding.jl returns an interval containing a unique fixed point, we return the midpoint of the interval as the actual fixed point. Naturally, limitations inherent to IntervalRootFinding.jl apply here.\n\nThe output of fixedpoints can be used in the BifurcationKit.jl as a start of a continuation process. See also periodicorbits.\n\nKeyword arguments\n\nmethod = IntervalRootFinding.Krawczyk configures the root finding method, see the docs of IntervalRootFinding.jl for all posibilities.\ntol = 1e-15 is the root-finding tolerance.\nwarn = true throw a warning if no fixed points are found.\n\n\n\n\n\n","category":"function"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"A rather simple example of the fixed points can be demonstrated using E.g., the Lorenz-63 system, whose fixed points can be calculated analytically to be the following three","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"(000) \nleft( sqrtbeta(rho-1) sqrtbeta(rho-1) rho-1 right) \nleft( -sqrtbeta(rho-1) -sqrtbeta(rho-1) rho-1 right) ","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"So, let's calculate","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"using ChaosTools\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\nfunction lorenz_jacob(u, p, t)\n    σ, ρ, β = p\n    return SMatrix{3,3}(-σ, ρ - u[3], u[2], σ, -1, u[1], 0, -u[1], -β)\nend\n\nρ, β = 30.0, 10/3\nlorenz = CoupledODEs(lorenz_rule, 10ones(3), [10.0, ρ, β])\n# Define the box within which to find fixed points:\nx = y = interval(-20, 20)\nz = interval(0, 40)\nbox = x × y × z\n\nfp, eigs, stable = fixedpoints(lorenz, box, lorenz_jacob)\nfp","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"and compare this with the analytic ones:","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"lorenzfp(ρ, β) = [\n    SVector(0, 0, 0.0),\n    SVector(sqrt(β*(ρ-1)), sqrt(β*(ρ-1)), ρ-1),\n    SVector(-sqrt(β*(ρ-1)), -sqrt(β*(ρ-1)), ρ-1),\n]\n\nlorenzfp(ρ, β)","category":"page"},{"location":"periodicity/#Stable-and-Unstable-Periodic-Orbits-of-Maps","page":"Fixed points & Periodicity","title":"Stable and Unstable Periodic Orbits of Maps","text":"","category":"section"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Chaotic behavior of low dimensional dynamical systems is affected by the position and the stability properties of the periodic orbits of a dynamical system.","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Finding unstable (or stable) periodic orbits of a discrete mapping analytically rapidly becomes impossible for higher orders of fixed points. Fortunately there is a numeric algorithm due to Schmelcher & Diakonos which allows such a computation. Notice that even though the algorithm can find stable fixed points, it is mainly aimed at unstable ones.","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"The functions periodicorbits and lambdamatrix implement the algorithm:","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"periodicorbits\nlambdamatrix\nlambdaperms","category":"page"},{"location":"periodicity/#ChaosTools.periodicorbits","page":"Fixed points & Periodicity","title":"ChaosTools.periodicorbits","text":"periodicorbits(ds::DeterministicIteratedMap,\n               o, ics [, λs, indss, singss]; kwargs...) -> FP\n\nFind fixed points FP of order o for the map ds using the algorithm due to Schmelcher & Diakonos[Schmelcher1997]. ics is a collection of initial conditions (container of vectors) to be evolved.\n\nOptional arguments\n\nThe optional arguments λs, indss, singss must be containers of appropriate values, besides λs which can also be a number. The elements of those containers are passed to: lambdamatrix(λ, inds, sings), which creates the appropriate mathbfLambda_k matrix. If these arguments are not given, a random permutation will be chosen for them, with λ=0.001.\n\nKeyword arguments\n\nmaxiters::Int = 100000: Maximum amount of iterations an i.c. will be iterated  before claiming it has not converged.\ndisttol = 1e-10: Distance tolerance. If the 2-norm of a previous state with  the next one is ≤ disttol then it has converged to a fixed point.\ninftol = 10.0: If a state reaches norm(state) ≥ inftol it is assumed that  it has escaped to infinity (and is thus abandoned).\nroundtol::Int = 4: The found fixed points are rounded  to roundtol digits before pushed into the list of returned fixed points FP,  if they are not already contained in FP.  This is done so that FP doesn't contain duplicate fixed points (notice  that this has nothing to do with disttol).\n\nDescription\n\nThe algorithm used can detect periodic orbits by turning fixed points of the original map ds to stable ones, through the transformation\n\nmathbfx_n+1 = mathbfx_n +\nmathbfLambda_kleft(f^(o)(mathbfx_n) - mathbfx_nright)\n\nThe index k counts the various possible mathbfLambda_k.\n\nPerformance notes\n\nAll initial conditions are evolved for all mathbfLambda_k which can very quickly lead to long computation times.\n\n[Schmelcher1997]: P. Schmelcher & F. K. Diakonos, Phys. Rev. Lett. 78, pp 4733 (1997)\n\n\n\n\n\n","category":"function"},{"location":"periodicity/#ChaosTools.lambdamatrix","page":"Fixed points & Periodicity","title":"ChaosTools.lambdamatrix","text":"lambdamatrix(λ, inds::Vector{Int}, sings) -> Λk\n\nReturn the matrix mathbfLambda_k used to create a new dynamical system with some unstable fixed points turned to stable in the function periodicorbits.\n\nArguments\n\nλ<:Real : the multiplier of the C_k matrix, with 0<λ<1.\ninds::Vector{Int} : The ith entry of this vector gives the row of the nonzero element of the ith column of C_k.\nsings::Vector{<:Real} : The element of the ith column of C_k is +1 if signs[i] > 0 and -1 otherwise (sings can also be Bool vector).\n\nCalling lambdamatrix(λ, D::Int) creates a random mathbfLambda_k by randomly generating an inds and a signs from all possible combinations. The collections of all these combinations can be obtained from the function lambdaperms.\n\nDescription\n\nEach element of inds must be unique such that the resulting matrix is orthogonal and represents the group of special reflections and permutations.\n\nDeciding the appropriate values for λ, inds, sings is not trivial. However, in ref.[Pingel2000] there is a lot of information that can help with that decision. Also, by appropriately choosing various values for λ, one can sort periodic orbits from e.g. least unstable to most unstable, see[Diakonos1998] for details.\n\n[Pingel2000]: D. Pingel et al., Phys. Rev. E 62, pp 2119 (2000)\n\n[Diakonos1998]: F. K. Diakonos et al., Phys. Rev. Lett. 81, pp 4349 (1998)\n\n\n\n\n\n","category":"function"},{"location":"periodicity/#ChaosTools.lambdaperms","page":"Fixed points & Periodicity","title":"ChaosTools.lambdaperms","text":"lambdaperms(D) -> indperms, singperms\n\nReturn two collections that each contain all possible combinations of indices (total of D) and signs (total of 2^D) for dimension D (see lambdamatrix).\n\n\n\n\n\n","category":"function"},{"location":"periodicity/#Standard-Map-example","page":"Fixed points & Periodicity","title":"Standard Map example","text":"","category":"section"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"For example, let's find the fixed points of the Systems.standardmap of order 2, 3, 4, 5, 6 and 8. We will use all permutations for the signs but only one for the inds. We will also only use one λ value, and a 21×21 density of initial conditions.","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"First, initialize everything","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"using ChaosTools\n\nfunction standardmap_rule(x, k, n)\n    theta = x[1]; p = x[2]\n    p += k[1]*sin(theta)\n    theta += p\n    return SVector(mod2pi(theta), mod2pi(p))\nend\n\nstandardmap = DeterministicIteratedMap(standardmap_rule, rand(2), [1.0])\nxs = range(0, stop = 2π, length = 11); ys = copy(xs)\nics = [SVector{2}(x,y) for x in xs for y in ys]\n\n# All permutations of [±1, ±1]:\nsingss = lambdaperms(2)[2] # second entry are the signs\n\n# I know from personal research I only need this `inds`:\nindss = [[1,2]] # <- must be container of vectors!\n\nλs = 0.005 # <- only this allowed to not be vector (could also be vector)\n\norders = [2, 3, 4, 5, 6, 8]\nALLFP = Dataset{2, Float64}[]\n\nstandardmap","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Then, do the necessary computations for all orders","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"for o in orders\n    FP = periodicorbits(standardmap, o, ics, λs, indss, singss)\n    push!(ALLFP, FP)\nend","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Plot the phase space of the standard map","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"using CairoMakie\niters = 1000\ndataset = trajectory(standardmap, iters)[1]\nfor x in xs\n    for y in ys\n        append!(dataset, trajectory(standardmap, iters, [x, y])[1])\n    end\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"\\theta\", ylabel = L\"p\",\n    limits = ((xs[1],xs[end]), (xs[1],xs[end]))\n)\nscatter!(ax, dataset[:, 1], dataset[:, 2]; markersize = 1, color = \"black\")\nfig","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"and finally, plot the fixed points","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"markers = [:diamond, :utriangle, :rect, :pentagon, :hexagon, :circle]\n\nfor i in 1:6\n    FP = ALLFP[i]\n    o = orders[i]\n    scatter!(ax, columns(FP)...; marker=markers[i], color = Cycled(i),\n        markersize = 30 - 2i, strokecolor = \"grey\", strokewidth = 1, label = \"order $o\"\n    )\nend\naxislegend(ax)\nfig","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Okay, this output is great, and we can tell that it is correct because:","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Fixed points of order n are also fixed points of order 2n 3n 4n \nBesides fixed points of previous orders, original fixed points of order n come in (possible multiples of) 2n-sized pairs (see e.g. order 5). This is a direct consequence of the Poincaré–Birkhoff theorem.","category":"page"},{"location":"periodicity/#Estimating-the-Period","page":"Fixed points & Periodicity","title":"Estimating the Period","text":"","category":"section"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"The function estimate_period offers ways for estimating the period (either exact for periodic timeseries, or approximate for near-periodic ones) of a given timeseries. We offer five methods to estimate periods, some of which work on evenly sampled data only, and others which accept any data. The figure below summarizes this: (Image: )","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"estimate_period\nyin","category":"page"},{"location":"periodicity/#ChaosTools.estimate_period","page":"Fixed points & Periodicity","title":"ChaosTools.estimate_period","text":"estimate_period(v::Vector, method, t=0:length(v)-1; kwargs...)\n\nEstimate the period of the signal v, with accompanying time vector t, using the given method.\n\nIf t is an AbstractArray, then it is iterated through to ensure that it's evenly sampled (if necessary for the algorithm).  To avoid this, you can pass any AbstractRange, like a UnitRange or a LinRange, which are defined to be evenly sampled.\n\nMethods requiring evenly sampled data\n\nThese methods are faster, but some are error-prone.\n\n:periodogram or :pg: Use the fast Fourier transform to compute a  periodogram (power-spectrum) of the given data.  Data must be evenly sampled.\n:multitaper or mt: The multitaper method reduces estimation bias by using multiple independent estimates from the same sample. Data tapers are then windowed and the power spectra are obtained.  Available keywords follow: nw is the time-bandwidth product, and ntapers is the number of tapers. If window is not specified, the signal is tapered with ntapers discrete prolate spheroidal sequences with time-bandwidth product nw. Each sequence is equally weighted; adaptive multitaper is not (yet) supported. If window is specified, each column is applied as a taper. The sum of periodograms is normalized by the total sum of squares of window.\n:autocorrelation or :ac: Use the autocorrelation function (AC). The value where the AC first comes back close to 1 is the period of the signal. The keyword L = length(v)÷10 denotes the length of the AC (thus, given the default setting, this method will fail if there less than 10 periods in the signal). The keyword ϵ = 0.2 (\\epsilon) means that 1-ϵ counts as \"1\" for the AC.\n:yin: The YIN algorithm. An autocorrelation-based method to estimate the fundamental period of the signal. See the original paper [CheveigneYIN2002] or the implementation yin. Sampling rate is taken as sr = 1/mean(diff(t)) if not given.\n\n[CheveigneYIN2002]: De Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency estimator for\n\nspeech and music. The Journal of the Acoustical Society of America, 111(4), 1917-1930.\n\nMethods not requiring evenly sampled data\n\nThese methods tend to be slow, but versatile and low-error.\n\n:lombscargle or :ls: Use the Lomb-Scargle algorithm to compute a periodogram.  The advantage of the Lomb-Scargle method is that it does not require an equally sampled dataset and performs well on undersampled datasets. Constraints have been set on the period, since Lomb-Scargle tends to have false peaks at very low frequencies.  That being said, it's a very flexible method.  It is extremely customizable, and the keyword arguments that can be passed to it are given in the documentation.\n:zerocrossing or :zc: Find the zero crossings of the data, and use the average difference between zero crossings as the period.  This is a naïve implementation, with only linear interpolation; however, it's useful as a sanity check.  The keyword line controls where the \"crossing point\" is. It defaults to mean(v).\n\nFor more information on the periodogram methods, see the documentation of DSP.jl and LombScargle.jl.\n\n\n\n\n\n","category":"function"},{"location":"periodicity/#ChaosTools.yin","page":"Fixed points & Periodicity","title":"ChaosTools.yin","text":"yin(sig::Vector, sr::Int; kwargs...) -> F0s, frame_times\n\nEstimate the fundamental frequency (F0) of the signal sig using the YIN algorithm [1]. The signal sig is a vector of points uniformly sampled at a rate sr.\n\nKeyword arguments\n\nw_len: size of the analysis window [samples == number of points]\nf_step: size of the lag between two consecutive frames [samples == number of points]\nf0_min: Minimum fundamental frequency that can be detected [linear frequency]\nf0_max: Maximum fundamental frequency that can be detected [linear frequency]\nharmonic_threshold: Threshold of detection. The algorithm returns the first minimum of the CMNDF function below this threshold.\ndiffference_function: The difference function to be used (by default ChaosTools.difference_function_original).\n\nDescription\n\nThe YIN algorithm [CheveigneYIN2002] estimates the signal's fundamental frequency F0 by basically looking for the period τ0  which minimizes the signal's autocorrelation. This autocorrelation is calculated for signal segments (frames), composed of two windows of length w_len. Each window is separated by a distance τ, and the idea is that the distance which minimizes the pairwise difference between each window is considered to be the fundamental period τ0 of that frame.\n\nMore precisely, the algorithm first computes the cumulative mean normalized difference function (MNDF) between two windows of a frame for several candidate periods τ ranging from τ_min=sr/f0_max to τ_max=sr/f0_min. The MNDF is defined as\n\nd_t^prime(tau) = begincases\n        1  textif  tau=0 \n        d_t(tau)left(frac 1 tau) sum_j=1^tau d_t(j)right  textotherwise\n        endcases\n\nwhere d_t is the difference function:\n\nd_t(tau) = sum_j=1^W (x_j - x_j+tau)^2\n\nIt then refines the local minima of the MNDF using parabolic (quadratic) interpolation. This is done by taking each minima, along with their first neighbor points, and finding the minimum of the corresponding interpolated parabola. The MNDF minima are substituted by the interpolation minima. Finally, the algorithm chooses the minimum with the smallest period and with a corresponding MNDF below the harmonic threshold. If this doesn't exist, it chooses the period corresponding to the global minimum. It repeats this for frames starting at the first signal point, and separated by a distance f_step (frames can overlap), and returns the vector of frequencies F0=sr/τ0 for each frame, along with the start times of each frame.\n\nAs a note, the physical unit of the frequency is 1/[time], where [time] is decided by the sampling rate sr. If, for instance, the sampling rate is over seconds, then the frequency is in Hertz.\n\n[CheveigneYIN2002]: De Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency estimator for\n\nspeech and music. The Journal of the Acoustical Society of America, 111(4), 1917-1930.\n\n\n\n\n\n","category":"function"},{"location":"periodicity/#Example","page":"Fixed points & Periodicity","title":"Example","text":"","category":"section"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Here we will use a modified FitzHugh-Nagumo system that results in periodic behavior, and then try to estimate its period. First, let's see the trajectory:","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"using ChaosTools, CairoMakie\n\nfunction FHN(u, p, t)\n    e, b, g = p\n    v, w = u\n    dv = min(max(-2 - v, v), 2 - v) - w\n    dw = e*(v - g*w + b)\n    return SVector(dv, dw)\nend\n\ng, e, b  = 0.8, 0.04, 0.0\np0 = [e, b, g]\n\nfhn = CoupledODEs(FHN, SVector(-2, -0.6667), p0)\nT, Δt = 1000.0, 0.1\nX, t = trajectory(fhn, T; Δt)\nv = X[:, 1]\n\nlines(t, v)","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"Examining the figure, one can see that the period of the system is around 91 time units. To estimate it numerically let's use some of the methods:","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"estimate_period(v, :autocorrelation, t)","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"estimate_period(v, :periodogram, t)","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"estimate_period(v, :zerocrossing, t)","category":"page"},{"location":"periodicity/","page":"Fixed points & Periodicity","title":"Fixed points & Periodicity","text":"estimate_period(v, :yin, t; f0_min=0.01)","category":"page"}]
}
